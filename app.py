import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
import google.generativeai as genai
import time
import random
import json
import altair as alt
import streamlit.components.v1 as components

# =========================
# 1) Page Config
# =========================
st.set_page_config(
    page_title="Google SERP æˆ°ç•¥é›·é” v3.0 (Enterprise)",
    page_icon="ğŸ¯",
    layout="wide"
)

st.title("ğŸ¯ Google SERP æˆ°ç•¥é›·é” v3.0 (Final)")
st.markdown("""
### Private SEO Weapon: Battlefield Reader & Content Architect
ä¸åƒ…æ˜¯åˆ†ææ„åœ–ï¼Œæ›´ç›´æ¥ç”Ÿæˆã€Œå¯è½åœ°çš„å…§å®¹ç­–ç•¥ã€èˆ‡ã€Œå¯«ä½œå¤§ç¶±ã€ã€‚å…·å‚™è‡ªå‹•ä¿®å¾© JSON èˆ‡æˆæœ¬ç›£æ§åŠŸèƒ½ã€‚
""")

# =========================
# 2) Sidebar Settings
# =========================
with st.sidebar:
    st.header("ğŸ”‘ å•Ÿå‹•é‡‘é‘°")
    st.info("è«‹ç¢ºä¿å·²å•Ÿç”¨ Google Custom Search API")
    GOOGLE_API_KEY = st.text_input("Google API Key", type="password")

    # [é˜²å‘†] è‡ªå‹•ç§»é™¤ cx=
    raw_cx = st.text_input("Search Engine ID (CX)", type="password")
    SEARCH_ENGINE_ID = raw_cx.replace("cx=", "").strip() if raw_cx else ""

    GEMINI_API_KEY = st.text_input("Gemini API Key", type="password")

    st.divider()
    st.header("ğŸ§  æ¨¡å‹è¨­å®š")
    MODEL_NAME = st.selectbox(
        "é¸æ“‡ä¸»è¦åˆ†ææ¨¡å‹",
        ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-3-pro-preview"],
        index=0,
        help="å»ºè­°ï¼šFlash è·‘å¤§é‡æ¸¬è©¦ï¼›Pro/3.0 åšæœ€çµ‚ç­–ç•¥ã€‚"
    )

    st.divider()
    st.header("ğŸŒ æˆ°å ´è¨­å®š")
    TARGET_GL = st.text_input("åœ°å€ (gl)", value="tw", help="ä¾‹å¦‚: tw, us, jp")
    TARGET_HL = st.text_input("èªè¨€ (hl)", value="zh-TW", help="ä¾‹å¦‚: zh-TW, en")
    MAX_PAGES = st.slider("æŠ“å–é æ•¸", 1, 3, 2, help="1é =Top10, 2é =Top20 (æ³¨æ„é…é¡æ¶ˆè€—)")

# =========================
# 2.1) CSE Preview (no quota)
# =========================
if SEARCH_ENGINE_ID:
    with st.expander("ğŸ‘€ æ‰‹å‹•æœå°‹é©—è­‰ (Google Programmable Search é è¦½)"):
        st.caption("æ­¤å€å¡Šä¸æ¶ˆè€— API é…é¡ï¼Œå¯ç›´æ¥é è¦½æ‚¨çš„ Custom Search Engine çµæœã€‚")
        components.html(
            f"""
            <script async src="https://cse.google.com/cse.js?cx={SEARCH_ENGINE_ID}"></script>
            <div class="gcse-search"></div>
            """,
            height=600,
            scrolling=True
        )

# =========================
# 3) Helpers
# =========================
def detect_page_type(item):
    """
    æ›´ç´°ç·»çš„é é¢é¡å‹åˆ¤æ–·ï¼ˆè¦å‰‡æ³•ï¼‰ã€‚
    """
    link = (item.get('link') or "").lower()
    snippet = (item.get('snippet') or "").lower()
    title = (item.get('title') or "").lower()

    # å¼·ç‰¹å¾µ
    if any(x in link for x in ['forum', 'ptt.cc', 'dcard.tw', 'mobile01', 'reddit', 'baha']):
        return "ğŸ—£ï¸ UGC/Forum (è«–å£‡)"
    if any(x in link for x in ['youtube.com', 'instagram.com', 'facebook.com', 'tiktok.com']):
        return "ğŸ¥ Social/Video (ç¤¾ç¾¤å½±éŸ³)"
    if any(x in link for x in ['/product/', 'shopee', 'momo', 'pchome', 'amazon', 'rakuten', 'buy123']):
        return "ğŸ›’ E-commerce (é›»å•†)"
    if any(x in link for x in ['/news/', 'news.', 'udn.com', 'ltn.com', 'chinatimes', 'ettoday']):
        return "ğŸ“° Media/News (æ–°èåª’é«”)"
    if ".gov" in link:
        return "ğŸ›ï¸ Government (æ”¿åºœ)"
    if "wiki" in link or "wikipedia" in link:
        return "ğŸ“– Wiki (ç™¾ç§‘)"

    # å¼±ç‰¹å¾µï¼ˆæ¨™é¡Œ/æ‘˜è¦ï¼‰
    if any(x in title for x in ['åƒ¹æ ¼', 'å„ªæƒ ', 'è²·', 'æŠ˜æ‰£', 'price', 'shop']) or any(x in snippet for x in ['åƒ¹æ ¼', 'å„ªæƒ ', 'æŠ˜æ‰£', 'è³¼è²·', 'ä¸‹å–®']):
        return "ğŸ›’ E-commerce (ç–‘ä¼¼é›»å•†)"
    if any(x in link for x in ['blog', 'article', 'post', 'topic']):
        return "ğŸ“ Blog/Article (å…§å®¹é )"

    return "ğŸ“„ General (ä¸€èˆ¬é é¢)"


@st.cache_data(ttl=3600, show_spinner=False)
def get_google_serp_data_cached(api_key, cx, keyword, gl, hl, pages):
    """
    å¿«å–ç‰ˆ SERP æŠ“å–å™¨ï¼ˆå›å‚³ list[dict] æˆ– {"error": "..."}ï¼‰
    """
    try:
        service = build("customsearch", "v1", developerKey=api_key)
    except Exception as e:
        return {"error": f"Service Build Error: {e}"}

    all_results = []

    for page in range(pages):
        start_index = (page * 10) + 1
        retries = 3

        while retries > 0:
            try:
                res = service.cse().list(
                    q=keyword,
                    cx=cx,
                    num=10,
                    start=start_index,
                    gl=gl,
                    hl=hl
                ).execute()

                items = res.get("items", [])
                if not items:
                    break

                for i, item in enumerate(items):
                    pagemap = item.get("pagemap", {}) or {}
                    metatags_list = pagemap.get("metatags", [{}]) or [{}]
                    metatags = metatags_list[0] if isinstance(metatags_list, list) and metatags_list else {}
                    description = metatags.get("og:description") or item.get("snippet") or ""

                    # æˆªæ–·ä»¥æ§ token
                    if description and len(description) > 200:
                        description = description[:200] + "..."

                    all_results.append({
                        "Rank": start_index + i,
                        "Type": detect_page_type(item),
                        "Title": item.get("title"),
                        "Description": description,
                        "DisplayLink": item.get("displayLink"),
                        "Link": item.get("link")
                    })
                break

            except Exception as e:
                retries -= 1
                wait_time = (3 - retries) * 2 + random.uniform(0, 1)
                time.sleep(wait_time)
                if retries == 0:
                    return {"error": f"API Fetch Error (Page {page+1}): {str(e)}"}

        time.sleep(1.2)

    return all_results


def _strip_code_fences(text: str) -> str:
    if not text:
        return ""
    t = text.strip()

    # å¸¸è¦‹æƒ…æ³ï¼š```json ... ```
    if t.startswith("```"):
        # ç§»é™¤ç¬¬ä¸€æ®µ fence è¡Œ
        first_newline = t.find("\n")
        if first_newline != -1:
            t = t[first_newline + 1:]
        # ç§»é™¤çµå°¾ fence
        if t.strip().endswith("```"):
            t = t.strip()[:-3]
    return t.strip()


def repair_json_with_gemini(api_key, broken_text, error_msg):
    """
    JSON ä¿®å¾©ï¼šä½¿ç”¨ä¾¿å®œ Flash æ¨¡å‹æŠŠå£æ‰çš„ JSON ä¿®æˆå¯ parse çš„ JSON å­—ä¸²
    å›å‚³ dict æˆ– None
    """
    genai.configure(api_key=api_key)
    repair_model = genai.GenerativeModel("gemini-2.5-flash")

    prompt = f"""
You are a JSON repair expert.
The following text was intended to be valid JSON but failed to parse.

Parse error:
{error_msg}

Broken text:
{broken_text}

Task:
- Return ONLY a valid JSON object string.
- Remove any markdown fences like ```json.
- Do not include any explanations.
"""

    try:
        response = repair_model.generate_content(prompt)
        cleaned = _strip_code_fences(getattr(response, "text", "") or "")
        if not cleaned:
            return None
        return json.loads(cleaned)
    except Exception:
        return None


def analyze_strategy_with_gemini(api_key, keyword, df, gl, model_name):
    """
    ä¸»ç­–ç•¥åˆ†æï¼šå›å‚³ (result_dict, raw_text)
    - å…ˆç”¨ä¸»æ¨¡å‹ç”¢ JSON
    - parse å¤±æ•— => å‘¼å« repair_json_with_gemini ä¿®å¾©
    """
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(model_name)

    # è¼¸å…¥å£“ç¸®ï¼šåªä¿ç•™å¿…è¦æ¬„ä½ï¼ˆæè¿°å·²åœ¨å‰é¢æˆªæ–·ï¼‰
    compact_df = df[["Rank", "Type", "Title", "Description", "DisplayLink"]].copy()

    # å†åšä¸€æ¬¡ä¿éšªæˆªæ–·ï¼ˆé¿å…æœ‰äººæ”¹å‰é¢é‚è¼¯ï¼‰
    compact_df["Title"] = compact_df["Title"].fillna("").astype(str).str.slice(0, 120)
    compact_df["Description"] = compact_df["Description"].fillna("").astype(str).str.slice(0, 220)
    compact_df["DisplayLink"] = compact_df["DisplayLink"].fillna("").astype(str).str.slice(0, 80)

    data_str = compact_df.to_string(index=False)

    prompt = f"""
ä½ æ˜¯å°ˆç²¾æ–¼ SEO çš„ç­–ç•¥é¡§å•èˆ‡å…§å®¹æ¶æ§‹å¸«ã€‚
æˆ‘å€‘æ­£åœ¨åˆ†æé—œéµå­—ã€Œ{keyword}ã€åœ¨ Google æœå°‹çµæœï¼ˆgl={gl}ï¼‰çš„å‰ {len(df)} ååˆ†ä½ˆã€‚

ä»¥ä¸‹æ˜¯ SERP è³‡æ–™ï¼ˆType ç‚ºè¦å‰‡åˆ¤æ–·çš„é é¢é¡å‹ï¼‰ï¼š
{data_str}

è«‹è¼¸å‡ºã€Œå¯è½åœ°ã€çš„ç­–ç•¥ï¼Œä¸¦åš´æ ¼ä»¥ JSON ç‰©ä»¶æ ¼å¼å›å‚³ï¼ˆä¸è¦ Markdown fenceï¼Œä¸è¦è§£é‡‹æ–‡å­—ï¼‰ï¼š

{{
  "User_Intent": "ä¸€å¥è©±èªªæ˜ä½¿ç”¨è€…æœ€æ ¸å¿ƒæƒ³å®Œæˆçš„ä»»å‹™ï¼ˆå¯å« 1-2 å€‹æ¬¡è¦æ„åœ–ï¼‰",
  "Battlefield_Status": "æˆ°å ´æ¦‚æ³ï¼šèª°åœ¨éœ¸æ¦œã€é å‹åˆ†ä½ˆã€æ˜¯å¦å£Ÿæ–·ã€UGC/åª’é«”/é›»å•†çš„æ¬Šé‡",
  "Opportunity_Gap": "ç›®å‰å‰æ®µçµæœçš„ä¸è¶³èˆ‡å¯åˆ‡å…¥çš„ç¼ºå£ï¼ˆè¦å…·é«”ï¼Œä¸è¦ç©ºè©±ï¼‰",
  "Recommended_Page_Type": "å»ºè­°æˆ‘å€‘è¦åšçš„é å‹ï¼ˆä¾‹å¦‚ï¼šæ¯”è¼ƒæ–‡/é¸è³¼æŒ‡å—/FAQ/ç”¢å“é /è½åœ°é /è©•æ¸¬ï¼‰",
  "Winning_Angles": [
    {{ "angle": "å·®ç•°åŒ–åˆ‡è§’1", "target_audience": "é©ç”¨å°è±¡/æƒ…å¢ƒ" }},
    {{ "angle": "å·®ç•°åŒ–åˆ‡è§’2", "target_audience": "é©ç”¨å°è±¡/æƒ…å¢ƒ" }},
    {{ "angle": "å·®ç•°åŒ–åˆ‡è§’3", "target_audience": "é©ç”¨å°è±¡/æƒ…å¢ƒ" }}
  ],
  "Killer_Titles": [
    {{ "title": "å¿…å‹æ¨™é¡Œ1", "reason": "ç‚ºä½•èƒ½è´ï¼ˆå°é½Šæ„åœ–/ç¼ºå£/å¯é»æ“Šï¼‰" }},
    {{ "title": "å¿…å‹æ¨™é¡Œ2", "reason": "ç‚ºä½•èƒ½è´ï¼ˆå°é½Šæ„åœ–/ç¼ºå£/å¯é»æ“Šï¼‰" }},
    {{ "title": "å¿…å‹æ¨™é¡Œ3", "reason": "ç‚ºä½•èƒ½è´ï¼ˆå°é½Šæ„åœ–/ç¼ºå£/å¯é»æ“Šï¼‰" }}
  ],
  "Content_Outline": [
    "H1: ...",
    "H2: ...",
    "H2: ...",
    "H3: ...",
    "H2: ...",
    "FAQ: ..."
  ]
}}
"""

    response = None
    raw_text = ""
    try:
        response = model.generate_content(prompt)
        raw_text = getattr(response, "text", "") or ""
        cleaned = _strip_code_fences(raw_text)
        parsed = json.loads(cleaned)
        return parsed, raw_text

    except json.JSONDecodeError as e:
        # è§¸ç™¼ä¿®å¾©
        repaired = repair_json_with_gemini(api_key, raw_text, str(e))
        if repaired is not None:
            return repaired, raw_text
        return {"error": f"JSON è§£æå¤±æ•—ä¸”ä¿®å¾©ç„¡æ•ˆ: {e}"}, raw_text

    except Exception as e:
        return {"error": f"API éŒ¯èª¤: {e}"}, raw_text


def generate_markdown_report(report_data_list):
    """ç”Ÿæˆäººé¡å¯è®€ Markdown å ±å‘Šï¼ˆé¡§å•äº¤ä»˜ç”¨ï¼‰"""
    md = f"# SEO æˆ°ç•¥åˆ†æå ±å‘Š\n\nç”Ÿæˆæ™‚é–“: {time.strftime('%Y-%m-%d %H:%M')}\n\n"

    for item in report_data_list:
        kw = item.get("Keyword", "")
        md += f"## é—œéµå­—ï¼š{kw}\n\n"

        md += "### 1. æ„åœ–èˆ‡æˆ°å ´\n"
        md += f"- **æ ¸å¿ƒæ„åœ–**: {item.get('User_Intent', '')}\n"
        md += f"- **æˆ°å ´ç¾æ³**: {item.get('Battlefield_Status', '')}\n"
        md += f"- **æ©Ÿæœƒç¼ºå£**: {item.get('Opportunity_Gap', '')}\n\n"

        md += "### 2. å…§å®¹ç­–ç•¥\n"
        md += f"- **å»ºè­°é å‹**: {item.get('Recommended_Page_Type', '')}\n"
        md += "- **è‡´å‹åˆ‡è§’**:\n"
        for angle in item.get("Winning_Angles", []):
            if isinstance(angle, dict):
                md += f"  - **{angle.get('angle', '')}**ï¼š{angle.get('target_audience', '')}\n"
            else:
                md += f"  - {str(angle)}\n"

        md += "\n### 3. å¿…å‹æ¨™é¡Œ\n"
        for t in item.get("Killer_Titles", []):
            if isinstance(t, dict):
                md += f"- {t.get('title', '')} (*{t.get('reason', '')}*)\n"
            else:
                md += f"- {str(t)}\n"

        md += "\n### 4. å…§å®¹å¤§ç¶± (Outline)\n"
        outline = item.get("Content_Outline", [])
        if isinstance(outline, list):
            for line in outline:
                md += f"- {line}\n"
        else:
            md += f"- {str(outline)}\n"

        md += "\n---\n\n"

    return md

# =========================
# 4) Main UI
# =========================
keywords_input = st.text_area(
    "è¼¸å…¥é—œéµå­— (è‡ªå‹•å»é‡)",
    height=100,
    placeholder="ç©ºæ°£æ¸…æ·¨æ©Ÿ æ¨è–¦\nCRM ç³»çµ±æ¯”è¼ƒ"
)

col_act1, col_act2 = st.columns([1, 3])
with col_act1:
    start_btn = st.button("ğŸš€ å•Ÿå‹•æˆ°ç•¥é›·é”", type="primary")

if start_btn:
    # Key æª¢æŸ¥
    if not (GOOGLE_API_KEY and SEARCH_ENGINE_ID and GEMINI_API_KEY):
        st.error("âš ï¸ è«‹å…ˆåœ¨å·¦å´æ¬„ä½è¼¸å…¥æ‰€æœ‰ API Key")
        st.stop()

    if not keywords_input.strip():
        st.warning("âš ï¸ è«‹è¼¸å…¥è‡³å°‘ä¸€å€‹é—œéµå­—")
        st.stop()

    # å»é‡ä¸”ä¿ç•™é †åº
    raw_keywords = [k.strip() for k in keywords_input.split("\n") if k.strip()]
    keywords = list(dict.fromkeys(raw_keywords))

    main_progress = st.progress(0)

    # æˆæœ¬/é…é¡ä¼°ç®—ï¼ˆç²—ç•¥ï¼šSERP callsï¼‰
    est_serp_calls = len(keywords) * MAX_PAGES
    st.caption(f"ğŸ“Š é è¨ˆåŸ·è¡Œï¼š{len(keywords)} å€‹é—œéµå­— | SERP æŸ¥è©¢æ¶ˆè€—ï¼šç´„ {est_serp_calls} æ¬¡ (Quota)")

    report_data_list = []

    for idx, kw in enumerate(keywords):
        st.subheader(f"ğŸ” ç›®æ¨™ï¼š{kw}")

        # 1) SERP (cached)
        with st.spinner(f"æ­£åœ¨æƒææˆ°å ´ (Top {MAX_PAGES*10})..."):
            raw_data = get_google_serp_data_cached(
                GOOGLE_API_KEY, SEARCH_ENGINE_ID, kw, TARGET_GL, TARGET_HL, MAX_PAGES
            )

        if isinstance(raw_data, dict) and "error" in raw_data:
            st.error(f"âŒ {raw_data['error']}")
        elif raw_data:
            df = pd.DataFrame(raw_data)

            # 1.1) Battlefield Viz
            with st.expander("ğŸ“Š æˆ°å ´åˆ†ä½ˆè¦–è¦ºåŒ– (é»æ“Šå±•é–‹)", expanded=True):
                col_viz1, col_viz2 = st.columns([2, 1])

                with col_viz1:
                    st.dataframe(
                        df[["Rank", "Type", "Title", "DisplayLink"]],
                        use_container_width=True,
                        height=220
                    )

                with col_viz2:
                    type_counts = df["Type"].value_counts().reset_index()
                    type_counts.columns = ["Type", "Count"]
                    chart = alt.Chart(type_counts).mark_arc(innerRadius=50).encode(
                        theta=alt.Theta(field="Count", type="quantitative"),
                        color=alt.Color(field="Type", type="nominal"),
                        tooltip=["Type", "Count"]
                    ).properties(title=f"Top {MAX_PAGES*10} é¡å‹ä½”æ¯”")
                    st.altair_chart(chart, use_container_width=True)

            # 2) AI Strategy
            with st.spinner(f"ğŸ§  {MODEL_NAME} æ­£åœ¨å»ºæ§‹ç­–ç•¥..."):
                analysis_result, raw_text = analyze_strategy_with_gemini(
                    GEMINI_API_KEY, kw, df, TARGET_GL, MODEL_NAME
                )

            if "error" in analysis_result:
                st.error(f"âŒ åˆ†æå¤±æ•—: {analysis_result['error']}")
                with st.expander("æŸ¥çœ‹åŸå§‹æ¨¡å‹å›æ‡‰ (Debug)"):
                    st.text(raw_text)
            else:
                st.markdown("#### ğŸ“ æˆ°ç•¥åˆ†æå ±å‘Š")

                c1, c2, c3 = st.columns(3)
                with c1:
                    st.info(f"**æ ¸å¿ƒæ„åœ–**\n\n{analysis_result.get('User_Intent', 'N/A')}")
                with c2:
                    st.warning(f"**å»ºè­°é å‹**\n\n{analysis_result.get('Recommended_Page_Type', 'N/A')}")
                with c3:
                    st.success(f"**æ©Ÿæœƒç¼ºå£**\n\n{analysis_result.get('Opportunity_Gap', 'N/A')}")

                t1, t2 = st.tabs(["ğŸ’¡ åˆ‡è§’èˆ‡æ¨™é¡Œ", "ğŸ§± å…§å®¹å¤§ç¶± (Outline)"])

                with t1:
                    st.markdown("**è‡´å‹åˆ‡è§’ï¼š**")
                    for a in analysis_result.get("Winning_Angles", []):
                        if isinstance(a, dict):
                            st.markdown(f"- **{a.get('angle', '')}**ï¼š{a.get('target_audience', '')}")
                        else:
                            st.markdown(f"- {str(a)}")

                    st.markdown("---")
                    st.markdown("**å¿…å‹æ¨™é¡Œï¼š**")
                    for t in analysis_result.get("Killer_Titles", []):
                        if isinstance(t, dict):
                            st.markdown(f"- {t.get('title', '')} (*{t.get('reason', '')}*)")
                        else:
                            st.markdown(f"- {str(t)}")

                with t2:
                    st.markdown("##### å»ºè­°æ–‡ç« çµæ§‹")
                    outline = analysis_result.get("Content_Outline", [])
                    if isinstance(outline, list):
                        st.text("\n".join([str(x) for x in outline]))
                    else:
                        st.text(str(outline))

                analysis_result["Keyword"] = kw
                report_data_list.append(analysis_result)

        else:
            st.error(f"âŒ ç„¡æ³•æŠ“å– {kw} çš„è³‡æ–™ (Unknown Error)")

        st.divider()
        main_progress.progress((idx + 1) / len(keywords))

    st.success("âœ… å…¨éƒ¨åˆ†æå®Œæˆï¼")

    # 3) Downloads
    if report_data_list:
        st.header("ğŸ“¥ ä¸‹è¼‰æˆ°ç•¥å ±å‘Š")

        md_report = generate_markdown_report(report_data_list)
        json_report = json.dumps(report_data_list, ensure_ascii=False, indent=2)

        # æ‰å¹³åŒ– CSV
        csv_data_list = []
        for item in report_data_list:
            flat_item = dict(item)
            flat_item["Winning_Angles"] = json.dumps(item.get("Winning_Angles", []), ensure_ascii=False)
            flat_item["Killer_Titles"] = json.dumps(item.get("Killer_Titles", []), ensure_ascii=False)
            outline = item.get("Content_Outline", [])
            flat_item["Content_Outline"] = "\n".join(outline) if isinstance(outline, list) else str(outline)
            csv_data_list.append(flat_item)
        df_csv = pd.DataFrame(csv_data_list)
        csv_report = df_csv.to_csv(index=False).encode("utf-8-sig")

        d1, d2, d3 = st.columns(3)
        with d1:
            st.download_button("ğŸ“„ ä¸‹è¼‰ Markdown å ±å‘Š", md_report, f"seo_report_{int(time.time())}.md", "text/markdown")
        with d2:
            st.download_button("ğŸ“Š ä¸‹è¼‰ Excel å‹å–„ CSV", csv_report, f"seo_data_{int(time.time())}.csv", "text/csv")
        with d3:
            st.download_button("ğŸ“‹ ä¸‹è¼‰ JSON", json_report, f"seo_raw_{int(time.time())}.json", "application/json")
