import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
import google.generativeai as genai
import time
import random
import json
import altair as alt
import streamlit.components.v1 as components
import io
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from collections import OrderedDict
import requests
from bs4 import BeautifulSoup
import html2text

# =================================================
# 0. å›ºå®šè¨­å®š
# =================================================
SEARCH_ENGINE_ID = "23e43fb5e029f4b50"  # CX å¯«æ­»ï¼ˆéæ©Ÿå¯†ï¼‰

# =================================================
# 1. Page Config
# =================================================
st.set_page_config(
    page_title="Google SERP æˆ°ç•¥é›·é” v4.0",
    page_icon="ğŸ¯",
    layout="wide"
)

st.title("ğŸ¯ Google SERP æˆ°ç•¥é›·é” v4.0")
st.markdown("""
### Private SEO Weapon: Two-Phase Strategic Analysis  
**ç¬¬ä¸€éšæ®µï¼šé—œéµå­—æ¢ç´¢** â†’ **ç¬¬äºŒéšæ®µï¼šSERP æˆ°ç•¥åˆ†æ + å…§å®¹æ–¹å‘æŒ‡å¼•**
""")

# =================================================
# 2. Sidebar
# =================================================
with st.sidebar:
    st.header("ğŸ”‘ API è¨­å®š")
    GOOGLE_API_KEY = st.text_input("Google API Key", type="password")
    GEMINI_API_KEY = st.text_input("Gemini API Key", type="password")

    st.divider()
    st.header("ğŸ§  æ¨¡å‹")
    MODEL_NAME = st.selectbox(
        "åˆ†ææ¨¡å‹",
        ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-3-pro-preview"],
        index=0
    )

    st.divider()
    st.header("ğŸŒ æœå°‹è¨­å®š")
    TARGET_GL = st.text_input("åœ°å€ (gl)", value="tw")
    TARGET_HL = st.text_input("èªè¨€ (hl)", value="zh-TW")
    MAX_PAGES = st.slider("æŠ“å–é æ•¸", 1, 3, 2)

    st.divider()
    st.header("âš¡ æ•ˆèƒ½è¨­å®š")
    MAX_CONCURRENT_SERP = st.slider(
        "SERP åŒæ™‚è«‹æ±‚æ•¸", 
        min_value=1, 
        max_value=5, 
        value=3,
        help="Google CSE API çš„ä¸¦ç™¼ä¸Šé™"
    )
    MAX_CONCURRENT_GEMINI = st.slider(
        "Gemini åŒæ™‚è«‹æ±‚æ•¸", 
        min_value=1, 
        max_value=3, 
        value=2,
        help="å»ºè­°ä¿å®ˆè¨­å®šï¼Œé¿å…æ’ RPM é™åˆ¶"
    )
    GEMINI_MIN_INTERVAL = st.slider(
        "Gemini è«‹æ±‚é–“éš”ï¼ˆç§’ï¼‰",
        min_value=0.5,
        max_value=3.0,
        value=1.0,
        step=0.5,
        help="æ¯æ¬¡ Gemini å‘¼å«çš„æœ€å°é–“éš”"
    )

# =================================================
# 3. Rate Limited Executorï¼ˆæ ¸å¿ƒå¹³è¡Œæ§åˆ¶ï¼‰
# =================================================
class RateLimitedExecutor:
    """å¸¶ rate limit çš„å¹³è¡ŒåŸ·è¡Œå™¨ï¼Œé˜²æ­¢ API éè¼‰"""
    
    def __init__(self, max_concurrent_serp=3, max_concurrent_gemini=2, gemini_min_interval=1.0):
        self.serp_semaphore = threading.Semaphore(max_concurrent_serp)
        self.gemini_semaphore = threading.Semaphore(max_concurrent_gemini)
        self.gemini_last_call = 0
        self.gemini_min_interval = gemini_min_interval
        self.lock = threading.Lock()
        
        # çµ±è¨ˆç”¨
        self.stats = {
            "serp_calls": 0,
            "gemini_calls": 0,
            "gemini_retries": 0,
            "errors": []
        }
    
    def call_serp(self, func, *args, **kwargs):
        """åŸ·è¡Œ SERP API å‘¼å«ï¼Œå¸¶ä¸¦ç™¼æ§åˆ¶"""
        with self.serp_semaphore:
            try:
                result = func(*args, **kwargs)
                with self.lock:
                    self.stats["serp_calls"] += 1
                time.sleep(0.5)  # åŸºæœ¬é–“éš”é¿å…éå¿«
                return result
            except Exception as e:
                with self.lock:
                    self.stats["errors"].append(f"SERP: {str(e)}")
                raise
    
    def call_gemini(self, func, *args, **kwargs):
        """åŸ·è¡Œ Gemini API å‘¼å«ï¼Œå¸¶ä¸¦ç™¼æ§åˆ¶ + é€Ÿç‡é™åˆ¶ + é‡è©¦"""
        with self.gemini_semaphore:
            # ç¢ºä¿æœ€å°é–“éš”
            with self.lock:
                elapsed = time.time() - self.gemini_last_call
                if elapsed < self.gemini_min_interval:
                    time.sleep(self.gemini_min_interval - elapsed)
                self.gemini_last_call = time.time()
            
            # Exponential backoff retry
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    result = func(*args, **kwargs)
                    with self.lock:
                        self.stats["gemini_calls"] += 1
                    return result
                except Exception as e:
                    error_str = str(e).lower()
                    is_rate_limit = any(x in error_str for x in ["429", "quota", "rate", "limit"])
                    
                    if is_rate_limit and attempt < max_retries - 1:
                        wait_time = (2 ** attempt) + random.uniform(0.5, 1.5)
                        with self.lock:
                            self.stats["gemini_retries"] += 1
                        time.sleep(wait_time)
                    else:
                        with self.lock:
                            self.stats["errors"].append(f"Gemini: {str(e)}")
                        raise
            
            # æœ€å¾Œä¸€æ¬¡å˜—è©¦
            return func(*args, **kwargs)


# =================================================
# 4. Phase 1: é—œéµå­—æ¢ç´¢ Helper Functions
# =================================================
def fetch_webpage_content(url):
    """æŠ“å–ç¶²é å…§å®¹ä¸¦è½‰æ›ç‚ºç´”æ–‡å­—"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        response.encoding = response.apparent_encoding or 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', 'noscript']):
            tag.decompose()
        
        # å–å¾—ä¸»è¦å…§å®¹
        main_content = soup.find('main') or soup.find('article') or soup.find('body')
        
        if main_content:
            # è½‰æ›ç‚ºç´”æ–‡å­—
            h = html2text.HTML2Text()
            h.ignore_links = True
            h.ignore_images = True
            h.ignore_emphasis = False
            text = h.handle(str(main_content))
            
            # æ¸…ç†å¤šé¤˜ç©ºç™½
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            cleaned_text = '\n'.join(lines)
            
            # é™åˆ¶é•·åº¦é¿å… token éå¤š
            if len(cleaned_text) > 15000:
                cleaned_text = cleaned_text[:15000] + "..."
            
            return cleaned_text, None
        else:
            return None, "ç„¡æ³•æ‰¾åˆ°ä¸»è¦å…§å®¹å€å¡Š"
            
    except requests.exceptions.RequestException as e:
        return None, f"ç¶²é æŠ“å–å¤±æ•—ï¼š{str(e)}"
    except Exception as e:
        return None, f"å…§å®¹è§£æéŒ¯èª¤ï¼š{str(e)}"


def extract_keywords_from_content(api_key, content, product_name, model_name):
    """AI åˆ†æé é¢å…§å®¹ï¼Œèƒå– 30 çµ„é—œéµå­—"""
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(model_name)
    
    prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ SEO é—œéµå­—ç ”ç©¶å°ˆå®¶ã€‚

è«‹åˆ†æä»¥ä¸‹ç¶²é å…§å®¹ï¼Œé‡å°ç”¢å“/æœå‹™ã€Œ{product_name}ã€èƒå– 30 çµ„å…·æœ‰ SEO åƒ¹å€¼çš„é—œéµå­—ã€‚

ç¶²é å…§å®¹ï¼š
---
{content}
---

è«‹å°‡é—œéµå­—åˆ†ç‚ºä¸‰é¡ï¼š
1. **ç—›é»å­—ï¼ˆPain Point Keywordsï¼‰**ï¼šä½¿ç”¨è€…å¯èƒ½é‡åˆ°çš„å•é¡Œã€å›°æ“¾ã€éœ€æ±‚ï¼ˆä¾‹å¦‚ï¼šã€Œå¤±çœ æ€éº¼è¾¦ã€ã€Œè‚©é ¸ç— ç—›ã€ï¼‰
2. **ç”¢å“å­—ï¼ˆProduct Keywordsï¼‰**ï¼šèˆ‡ç”¢å“/æœå‹™ç›´æ¥ç›¸é—œçš„æœå°‹è©ï¼ˆä¾‹å¦‚ï¼šã€Œä¿å¥é£Ÿå“æ¨è–¦ã€ã€ŒæŒ‰æ‘©æ¤…åŠŸèƒ½ã€ï¼‰
3. **å“ç‰Œå­—ï¼ˆBrand Keywordsï¼‰**ï¼šå“ç‰Œåç¨±ã€ç«¶å“åç¨±ã€å•†å“å‹è™Ÿï¼ˆä¾‹å¦‚ï¼šã€ŒXXXå“ç‰Œè©•åƒ¹ã€ã€ŒYYY vs ZZZã€ï¼‰

æ¯é¡è‡³å°‘ 8 çµ„ï¼Œç¸½å…± 30 çµ„é—œéµå­—ã€‚

è«‹åªç”¨ JSON å›å‚³ï¼Œä¸è¦ä»»ä½• markdown æ ¼å¼ã€ä¸è¦ ```json```ã€ä¸è¦ä»»ä½•å‰å¾Œèªªæ˜æ–‡å­—ï¼š
{{
  "pain_point_keywords": [
    {{"keyword": "é—œéµå­—1", "search_intent": "æœå°‹æ„åœ–èªªæ˜"}},
    {{"keyword": "é—œéµå­—2", "search_intent": "æœå°‹æ„åœ–èªªæ˜"}}
  ],
  "product_keywords": [
    {{"keyword": "é—œéµå­—1", "search_intent": "æœå°‹æ„åœ–èªªæ˜"}},
    {{"keyword": "é—œéµå­—2", "search_intent": "æœå°‹æ„åœ–èªªæ˜"}}
  ],
  "brand_keywords": [
    {{"keyword": "é—œéµå­—1", "search_intent": "æœå°‹æ„åœ–èªªæ˜"}},
    {{"keyword": "é—œéµå­—2", "search_intent": "æœå°‹æ„åœ–èªªæ˜"}}
  ]
}}
"""
    
    try:
        res = model.generate_content(prompt)
        raw = res.text.strip()
        cleaned = raw.replace("```json", "").replace("```", "").strip()
        return json.loads(cleaned), None
    except json.JSONDecodeError as e:
        # å˜—è©¦ä¿®å¾©
        fixed = repair_json(api_key, raw, str(e))
        if fixed:
            return fixed, None
        return None, f"JSON è§£æå¤±æ•—ï¼š{str(e)}"
    except Exception as e:
        return None, f"AI åˆ†æå¤±æ•—ï¼š{str(e)}"


def get_related_keywords_from_serp(api_key, keyword, gl, hl):
    """ä½¿ç”¨ Google CSE æŸ¥è©¢é—œéµå­—ï¼Œå¾çµæœä¸­æå–ç›¸é—œè©å½™"""
    try:
        service = build("customsearch", "v1", developerKey=api_key)
        res = service.cse().list(
            q=keyword,
            cx=SEARCH_ENGINE_ID,
            num=10,
            gl=gl,
            hl=hl
        ).execute()
        
        related = []
        
        # å¾æ¨™é¡Œå’Œæè¿°ä¸­æå–ç›¸é—œè©
        items = res.get("items", [])
        for item in items[:5]:  # åªå–å‰ 5 å€‹çµæœ
            title = item.get("title", "")
            snippet = item.get("snippet", "")
            # ç°¡å–®åˆä½µä½œç‚ºåƒè€ƒ
            related.append(f"{title[:50]}")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ spelling å»ºè­°
        if "spelling" in res:
            related.insert(0, res["spelling"].get("correctedQuery", ""))
        
        return related[:5], None
        
    except Exception as e:
        return [], str(e)


# =================================================
# 5. Phase 2: SERP åˆ†æ Helper Functions
# =================================================
def detect_page_type(item):
    """åˆ¤æ–· SERP çµæœçš„é é¢é¡å‹"""
    link = (item.get("link") or "").lower()
    title = (item.get("title") or "").lower()

    if any(x in link for x in ["ptt.cc", "dcard", "reddit", "mobile01"]):
        return "UGC / Forum"
    if any(x in link for x in ["youtube.com", "instagram.com", "tiktok.com"]):
        return "Social / Video"
    if any(x in link for x in ["shopee", "momo", "pchome", "amazon", "/product/"]):
        return "E-commerce"
    if any(x in link for x in ["udn.com", "ltn.com", "ettoday", "/news/"]):
        return "Media"
    if "wiki" in link:
        return "Wiki"
    if any(x in title for x in ["åƒ¹æ ¼", "å„ªæƒ ", "æ¨è–¦"]):
        return "Commercial Content"
    return "General"


def get_serp_raw(api_key, keyword, gl, hl, pages):
    """æŠ“å– SERP è³‡æ–™"""
    service = build("customsearch", "v1", developerKey=api_key)
    results = []

    for page in range(pages):
        start = page * 10 + 1
        res = service.cse().list(
            q=keyword,
            cx=SEARCH_ENGINE_ID,
            num=10,
            start=start,
            gl=gl,
            hl=hl
        ).execute()

        for i, item in enumerate(res.get("items", [])):
            desc = item.get("snippet", "") or ""
            if len(desc) > 200:
                desc = desc[:200] + "..."

            results.append({
                "Rank": start + i,
                "Type": detect_page_type(item),
                "Title": item.get("title"),
                "Description": desc,
                "DisplayLink": item.get("displayLink"),
                "URL": item.get("link")
            })

        if page < pages - 1:
            time.sleep(0.8)

    return results


def repair_json(api_key, broken_text, error):
    """å˜—è©¦ä¿®å¾© Gemini å›å‚³çš„å£ JSON"""
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-2.0-flash")

    prompt = f"""
Fix the JSON below and return ONLY valid JSON. No markdown, no explanation.

Error: {error}

Broken JSON:
{broken_text}
"""
    try:
        res = model.generate_content(prompt)
        text = res.text.strip()
        text = text.replace("```json", "").replace("```", "").strip()
        return json.loads(text)
    except Exception:
        return None


def analyze_strategy_raw(api_key, keyword, df, gl, model_name):
    """åŸ·è¡Œ Gemini ç­–ç•¥åˆ†æ"""
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(model_name)

    data = df[["Rank", "Type", "Title", "Description", "DisplayLink"]].to_string(index=False)

    prompt = f"""
ä½ æ˜¯ SEO ç­–ç•¥é¡§å•ã€‚
è«‹åˆ†æé—œéµå­—ã€Œ{keyword}ã€åœ¨ Googleï¼ˆ{gl}ï¼‰çš„ SERP æˆ°å ´ã€‚

è³‡æ–™ï¼š
{data}

è«‹åªç”¨ JSON å›å‚³ï¼Œä¸è¦ä»»ä½• markdown æ ¼å¼ã€ä¸è¦ ```json```ã€ä¸è¦ä»»ä½•å‰å¾Œèªªæ˜æ–‡å­—ï¼š
{{
  "User_Intent": "æè¿°ä½¿ç”¨è€…æœå°‹æ­¤é—œéµå­—çš„æ„åœ–",
  "Battlefield_Status": "ç›®å‰ SERP æˆ°å ´çš„ç«¶çˆ­ç‹€æ…‹åˆ†æ",
  "Opportunity_Gap": "ç™¼ç¾çš„æ©Ÿæœƒç¼ºå£",
  "Recommended_Page_Type": "å»ºè­°è£½ä½œçš„é é¢é¡å‹",
  "Winning_Angles": [
    {{ "angle": "åˆ‡è§’1", "target": "ç›®æ¨™å—çœ¾" }},
    {{ "angle": "åˆ‡è§’2", "target": "ç›®æ¨™å—çœ¾" }}
  ],
  "Killer_Titles": [
    {{ "title": "æ¨™é¡Œ1", "reason": "ç‚ºä½•æœ‰æ•ˆ" }},
    {{ "title": "æ¨™é¡Œ2", "reason": "ç‚ºä½•æœ‰æ•ˆ" }}
  ]
}}
"""

    try:
        res = model.generate_content(prompt)
        raw = res.text.strip()
        cleaned = raw.replace("```json", "").replace("```", "").strip()
        return json.loads(cleaned), raw
    except json.JSONDecodeError as e:
        fixed = repair_json(api_key, raw, str(e))
        if fixed:
            return fixed, raw
        return {"error": str(e), "raw_response": raw}, raw
    except Exception as e:
        return {"error": str(e)}, str(e)


def generate_content_direction(api_key, all_strategies, selected_keywords, model_name):
    """æ ¹æ“šæ‰€æœ‰é—œéµå­—çš„ SERP åˆ†æï¼Œç”¢ç”Ÿå…§å®¹å¯«ä½œç¶œåˆæŒ‡å¼•"""
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(model_name)
    
    # æ•´ç†æ‰€æœ‰ç­–ç•¥è³‡è¨Š
    strategy_summary = []
    for s in all_strategies:
        if "error" not in s:
            strategy_summary.append({
                "keyword": s.get("Keyword", ""),
                "intent": s.get("User_Intent", ""),
                "opportunity": s.get("Opportunity_Gap", ""),
                "page_type": s.get("Recommended_Page_Type", "")
            })
    
    prompt = f"""
ä½ æ˜¯ä¸€ä½è³‡æ·±å…§å®¹ç­–ç•¥é¡§å•ã€‚

æ ¹æ“šä»¥ä¸‹é—œéµå­—çš„ SERP æˆ°å ´åˆ†æçµæœï¼Œè«‹ç”¢ç”Ÿä¸€ä»½ã€Œå…§å®¹å¯«ä½œæ–¹å‘ç¶œåˆæŒ‡å¼•ã€ã€‚

åˆ†æçš„é—œéµå­—ï¼š
{json.dumps(selected_keywords, ensure_ascii=False)}

å„é—œéµå­—çš„ SERP åˆ†ææ‘˜è¦ï¼š
{json.dumps(strategy_summary, ensure_ascii=False, indent=2)}

è«‹æä¾›å…·é«”ã€å¯åŸ·è¡Œçš„å…§å®¹ç­–ç•¥å»ºè­°ã€‚

è«‹åªç”¨ JSON å›å‚³ï¼Œä¸è¦ä»»ä½• markdown æ ¼å¼ã€ä¸è¦ ```json```ã€ä¸è¦ä»»ä½•å‰å¾Œèªªæ˜æ–‡å­—ï¼š
{{
  "content_theme": "æ ¸å¿ƒä¸»é¡Œæ–¹å‘ï¼ˆä¸€å¥è©±æè¿°é€™ç¯‡å…§å®¹çš„æ ¸å¿ƒå®šä½ï¼‰",
  "target_audience": "ç›®æ¨™å—çœ¾æè¿°ï¼ˆä»–å€‘æ˜¯èª°ï¼Ÿåœ¨ä»€éº¼æƒ…å¢ƒä¸‹æœƒæœå°‹ï¼Ÿï¼‰",
  "content_structure": [
    {{"section": "å»ºè­°æ®µè½æ¨™é¡Œ1", "focus": "é€™æ®µè¦æ¶µè“‹çš„é‡é»å…§å®¹", "keywords_to_use": ["å»ºè­°ä½¿ç”¨çš„é—œéµå­—"]}},
    {{"section": "å»ºè­°æ®µè½æ¨™é¡Œ2", "focus": "é€™æ®µè¦æ¶µè“‹çš„é‡é»å…§å®¹", "keywords_to_use": ["å»ºè­°ä½¿ç”¨çš„é—œéµå­—"]}},
    {{"section": "å»ºè­°æ®µè½æ¨™é¡Œ3", "focus": "é€™æ®µè¦æ¶µè“‹çš„é‡é»å…§å®¹", "keywords_to_use": ["å»ºè­°ä½¿ç”¨çš„é—œéµå­—"]}}
  ],
  "must_cover_topics": ["å¿…é ˆæ¶µè“‹çš„ä¸»é¡Œ1", "å¿…é ˆæ¶µè“‹çš„ä¸»é¡Œ2", "å¿…é ˆæ¶µè“‹çš„ä¸»é¡Œ3"],
  "differentiation_angle": "å·®ç•°åŒ–åˆ‡è§’ï¼ˆå¦‚ä½•è®“é€™ç¯‡å…§å®¹èˆ‡ç¾æœ‰ SERP çµæœä¸åŒï¼‰",
  "content_format_suggestion": "å»ºè­°çš„å…§å®¹æ ¼å¼ï¼ˆä¾‹å¦‚ï¼šæ¯”è¼ƒè¡¨ã€æ­¥é©Ÿæ•™å­¸ã€æ¡ˆä¾‹åˆ†æç­‰ï¼‰",
  "avoid_pitfalls": ["éœ€é¿å…çš„å¯«ä½œé™·é˜±1", "éœ€é¿å…çš„å¯«ä½œé™·é˜±2"]
}}
"""
    
    try:
        res = model.generate_content(prompt)
        raw = res.text.strip()
        cleaned = raw.replace("```json", "").replace("```", "").strip()
        return json.loads(cleaned), None
    except json.JSONDecodeError as e:
        fixed = repair_json(api_key, raw, str(e))
        if fixed:
            return fixed, None
        return None, f"JSON è§£æå¤±æ•—ï¼š{str(e)}"
    except Exception as e:
        return None, f"å…§å®¹æŒ‡å¼•ç”¢ç”Ÿå¤±æ•—ï¼š{str(e)}"


def process_single_keyword(kw, executor, google_key, gemini_key, gl, hl, pages, model_name):
    """è™•ç†å–®ä¸€é—œéµå­—çš„å®Œæ•´æµç¨‹ï¼ˆSERP + åˆ†æï¼‰"""
    result = {
        "keyword": kw,
        "serp_df": None,
        "serp_raw": None,
        "strategy": None,
        "raw_response": None,
        "error": None,
        "timing": {}
    }
    
    try:
        # Step 1: SERP æŠ“å–
        start_serp = time.time()
        serp_data = executor.call_serp(
            get_serp_raw, google_key, kw, gl, hl, pages
        )
        result["timing"]["serp"] = time.time() - start_serp
        result["serp_raw"] = serp_data
        result["serp_df"] = pd.DataFrame(serp_data)
        
        # Step 2: Gemini åˆ†æ
        start_gemini = time.time()
        strategy, raw = executor.call_gemini(
            analyze_strategy_raw, gemini_key, kw, result["serp_df"], gl, model_name
        )
        result["timing"]["gemini"] = time.time() - start_gemini
        result["strategy"] = strategy
        result["raw_response"] = raw
        
    except Exception as e:
        result["error"] = str(e)
    
    return result


# =================================================
# 6. Session State åˆå§‹åŒ–
# =================================================
if "phase1_keywords" not in st.session_state:
    st.session_state.phase1_keywords = None
if "selected_keywords" not in st.session_state:
    st.session_state.selected_keywords = []
if "phase1_completed" not in st.session_state:
    st.session_state.phase1_completed = False


# =================================================
# 7. Main App - å…©éšæ®µåˆ†é 
# =================================================
tab1, tab2 = st.tabs(["ğŸ” ç¬¬ä¸€éšæ®µï¼šé—œéµå­—æ¢ç´¢", "ğŸ“Š ç¬¬äºŒéšæ®µï¼šSERP æˆ°ç•¥åˆ†æ"])

# =================================================
# 7.1 ç¬¬ä¸€éšæ®µï¼šé—œéµå­—æ¢ç´¢
# =================================================
with tab1:
    st.markdown("""
    ### ğŸ” é—œéµå­—æ¢ç´¢
    è¼¸å…¥ç”¢å“é é¢ç¶²å€ï¼ŒAI å°‡è‡ªå‹•èƒå–é«˜åƒ¹å€¼é—œéµå­—ä¸¦æ“´å±•ç›¸é—œè©å½™ã€‚
    """)
    
    col1, col2 = st.columns([2, 1])
    with col1:
        input_url = st.text_input(
            "ç¶²é ç¶²å€",
            placeholder="https://example.com/product-page",
            help="è¼¸å…¥ç”¢å“æˆ–æœå‹™é é¢çš„ç¶²å€"
        )
    with col2:
        product_name = st.text_input(
            "ç”¢å“/æœå‹™åç¨±",
            placeholder="ä¾‹ï¼šç›Šç”ŸèŒä¿å¥é£Ÿå“",
            help="ç”¨æ–¼å¼•å° AI èƒå–æ›´ç²¾æº–çš„é—œéµå­—"
        )
    
    # å‚™ç”¨æ–¹æ¡ˆï¼šç›´æ¥è²¼ä¸Šå…§å®¹
    with st.expander("ğŸ“ å‚™ç”¨æ–¹æ¡ˆï¼šç›´æ¥è²¼ä¸Šç¶²é å…§å®¹"):
        manual_content = st.text_area(
            "è²¼ä¸Šç¶²é å…§å®¹ï¼ˆè‹¥ç¶²å€ç„¡æ³•æŠ“å–æ™‚ä½¿ç”¨ï¼‰",
            height=200,
            placeholder="å°‡ç¶²é çš„ä¸»è¦æ–‡å­—å…§å®¹è²¼åœ¨é€™è£¡..."
        )
    
    if st.button("ğŸš€ é–‹å§‹é—œéµå­—æ¢ç´¢", type="primary", key="phase1_btn"):
        if not (GOOGLE_API_KEY and GEMINI_API_KEY):
            st.error("è«‹å…ˆåœ¨å´é‚Šæ¬„è¼¸å…¥ Google API Key èˆ‡ Gemini API Key")
            st.stop()
        
        if not product_name:
            st.warning("è«‹è¼¸å…¥ç”¢å“/æœå‹™åç¨±")
            st.stop()
        
        content = None
        
        # å„ªå…ˆä½¿ç”¨ç¶²å€æŠ“å–
        if input_url:
            with st.spinner("ğŸŒ æ­£åœ¨æŠ“å–ç¶²é å…§å®¹..."):
                content, error = fetch_webpage_content(input_url)
                if error:
                    st.warning(f"âš ï¸ {error}")
                    if manual_content:
                        st.info("ä½¿ç”¨æ‚¨è²¼ä¸Šçš„å…§å®¹ç¹¼çºŒåˆ†æ...")
                        content = manual_content
        elif manual_content:
            content = manual_content
        
        if not content:
            st.error("è«‹è¼¸å…¥ç¶²å€æˆ–è²¼ä¸Šç¶²é å…§å®¹")
            st.stop()
        
        # é¡¯ç¤ºæŠ“å–åˆ°çš„å…§å®¹é è¦½
        with st.expander("ğŸ“„ æŠ“å–åˆ°çš„å…§å®¹é è¦½", expanded=False):
            st.text(content[:2000] + "..." if len(content) > 2000 else content)
        
        # AI èƒå–é—œéµå­—
        with st.spinner("ğŸ¤– AI æ­£åœ¨åˆ†æä¸¦èƒå–é—œéµå­—..."):
            keywords_data, error = extract_keywords_from_content(
                GEMINI_API_KEY, content, product_name, MODEL_NAME
            )
        
        if error:
            st.error(f"âŒ {error}")
            st.stop()
        
        # ç‚ºæ¯å€‹é—œéµå­—å–å¾—ç›¸é—œè©
        st.info("ğŸ”„ æ­£åœ¨æ“´å±•é—œè¯å­—å½™...")
        progress_bar = st.progress(0)
        
        all_keywords = []
        categories = ["pain_point_keywords", "product_keywords", "brand_keywords"]
        category_names = {"pain_point_keywords": "ç—›é»å­—", "product_keywords": "ç”¢å“å­—", "brand_keywords": "å“ç‰Œå­—"}
        
        total_kw = sum(len(keywords_data.get(cat, [])) for cat in categories)
        processed = 0
        
        for category in categories:
            kw_list = keywords_data.get(category, [])
            for kw_item in kw_list:
                keyword = kw_item.get("keyword", "")
                if keyword:
                    related, _ = get_related_keywords_from_serp(
                        GOOGLE_API_KEY, keyword, TARGET_GL, TARGET_HL
                    )
                    all_keywords.append({
                        "category": category,
                        "category_name": category_names[category],
                        "keyword": keyword,
                        "search_intent": kw_item.get("search_intent", ""),
                        "related": related
                    })
                    processed += 1
                    progress_bar.progress(processed / total_kw)
                    time.sleep(0.3)  # é¿å… API éè¼‰
        
        progress_bar.empty()
        st.session_state.phase1_keywords = all_keywords
        st.success(f"âœ… æˆåŠŸèƒå– {len(all_keywords)} çµ„é—œéµå­—ï¼")
    
    # é¡¯ç¤ºé—œéµå­—çµæœ
    if st.session_state.phase1_keywords:
        st.divider()
        st.subheader("ğŸ“‹ èƒå–çµæœï¼šè«‹å‹¾é¸è¦ç”¨æ–¼ç¬¬äºŒéšæ®µçš„é—œéµå­—")
        
        keywords = st.session_state.phase1_keywords
        
        # æŒ‰é¡åˆ¥åˆ†çµ„é¡¯ç¤º
        for category, cat_name, color in [
            ("pain_point_keywords", "ğŸ”´ ç—›é»å­—", "#FFE4E1"),
            ("product_keywords", "ğŸŸ¢ ç”¢å“å­—", "#E8F5E9"),
            ("brand_keywords", "ğŸ”µ å“ç‰Œå­—", "#E3F2FD")
        ]:
            cat_keywords = [k for k in keywords if k["category"] == category]
            if cat_keywords:
                st.markdown(f"### {cat_name}")
                
                for kw in cat_keywords:
                    with st.container():
                        cols = st.columns([0.5, 3, 4])
                        with cols[0]:
                            checked = st.checkbox(
                                "é¸",
                                key=f"kw_{kw['keyword']}",
                                label_visibility="collapsed"
                            )
                        with cols[1]:
                            st.markdown(f"**{kw['keyword']}**")
                            st.caption(kw.get("search_intent", ""))
                        with cols[2]:
                            if kw.get("related"):
                                st.caption("é—œè¯å­—ï¼š" + " â€¢ ".join(kw["related"][:4]))
        
        st.divider()
        
        # æ”¶é›†å‹¾é¸çš„é—œéµå­—
        selected = []
        for kw in keywords:
            if st.session_state.get(f"kw_{kw['keyword']}", False):
                selected.append(kw["keyword"])
        
        col1, col2, col3 = st.columns([2, 1, 1])
        with col1:
            st.info(f"å·²é¸æ“‡ {len(selected)} çµ„é—œéµå­—")
        with col2:
            if st.button("âœ… å…¨é¸"):
                for kw in keywords:
                    st.session_state[f"kw_{kw['keyword']}"] = True
                st.rerun()
        with col3:
            if st.button("âŒ å…¨ä¸é¸"):
                for kw in keywords:
                    st.session_state[f"kw_{kw['keyword']}"] = False
                st.rerun()
        
        if st.button("ğŸ¯ é€²å…¥ç¬¬äºŒéšæ®µåˆ†æ", type="primary", disabled=len(selected) == 0):
            st.session_state.selected_keywords = selected
            st.session_state.phase1_completed = True
            st.success("âœ… å·²å°‡é—œéµå­—å‚³éè‡³ç¬¬äºŒéšæ®µï¼Œè«‹åˆ‡æ›åˆ†é ï¼")


# =================================================
# 7.2 ç¬¬äºŒéšæ®µï¼šSERP æˆ°ç•¥åˆ†æ
# =================================================
with tab2:
    st.markdown("""
    ### ğŸ“Š SERP æˆ°ç•¥åˆ†æ
    é‡å°é—œéµå­—é€²è¡Œæœå°‹çµæœåˆ†æï¼Œç”¢å‡ºç­–ç•¥å»ºè­°èˆ‡å…§å®¹å¯«ä½œæ–¹å‘æŒ‡å¼•ã€‚
    """)
    
    # Google CSE é è¦½
    with st.expander("ğŸ‘€ Google æœå°‹é è¦½ï¼ˆä¸è€— APIï¼‰"):
        components.html(
            f"""
            <script async src="https://cse.google.com/cse.js?cx={SEARCH_ENGINE_ID}"></script>
            <div class="gcse-search"></div>
            """,
            height=600,
            scrolling=True
        )
    
    # é—œéµå­—è¼¸å…¥
    if st.session_state.phase1_completed and st.session_state.selected_keywords:
        default_keywords = "\n".join(st.session_state.selected_keywords)
        st.success(f"âœ… å·²å¾ç¬¬ä¸€éšæ®µæ¥æ”¶ {len(st.session_state.selected_keywords)} çµ„é—œéµå­—")
    else:
        default_keywords = ""
    
    keywords_input = st.text_area(
        "è¼¸å…¥é—œéµå­—ï¼ˆæ¯è¡Œä¸€å€‹ï¼Œè‡ªå‹•å»é‡ï¼‰",
        value=default_keywords,
        height=150,
        placeholder="ç©ºæ°£æ¸…æ·¨æ©Ÿ æ¨è–¦\nCRM ç³»çµ±æ¯”è¼ƒ\nè¾¦å…¬æ¤… ptt"
    )
    
    # é¡¯ç¤ºé ä¼°è³‡è¨Š
    if keywords_input.strip():
        keywords_preview = list(dict.fromkeys([k.strip() for k in keywords_input.split("\n") if k.strip()]))
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("é—œéµå­—æ•¸", len(keywords_preview))
        with col2:
            st.metric("é ä¼° SERP å‘¼å«", len(keywords_preview) * MAX_PAGES)
        with col3:
            st.metric("é ä¼° Gemini å‘¼å«", len(keywords_preview) + 1)  # +1 for content direction
    
    if st.button("ğŸš€ å•Ÿå‹•æˆ°ç•¥åˆ†æ", type="primary", key="phase2_btn"):
        if not (GOOGLE_API_KEY and GEMINI_API_KEY):
            st.error("è«‹è¼¸å…¥ Google API Key èˆ‡ Gemini API Key")
            st.stop()

        keywords = list(dict.fromkeys([k.strip() for k in keywords_input.split("\n") if k.strip()]))
        
        if not keywords:
            st.warning("è«‹è¼¸å…¥è‡³å°‘ä¸€å€‹é—œéµå­—")
            st.stop()

        # åˆå§‹åŒ–åŸ·è¡Œå™¨
        executor = RateLimitedExecutor(
            max_concurrent_serp=MAX_CONCURRENT_SERP,
            max_concurrent_gemini=MAX_CONCURRENT_GEMINI,
            gemini_min_interval=GEMINI_MIN_INTERVAL
        )
        
        # UI å…ƒç´ 
        st.divider()
        status_header = st.empty()
        status_header.info(f"âš¡ å¹³è¡Œè™•ç†ä¸­... SERPÃ—{MAX_CONCURRENT_SERP} / GeminiÃ—{MAX_CONCURRENT_GEMINI}")
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # æ”¶é›†çµæœ
        all_results = OrderedDict()
        completed_count = 0
        total_start_time = time.time()
        
        # å¹³è¡ŒåŸ·è¡Œ
        max_workers = max(MAX_CONCURRENT_SERP, MAX_CONCURRENT_GEMINI) + 1
        
        with ThreadPoolExecutor(max_workers=max_workers) as pool:
            future_to_kw = {
                pool.submit(
                    process_single_keyword,
                    kw, executor, GOOGLE_API_KEY, GEMINI_API_KEY,
                    TARGET_GL, TARGET_HL, MAX_PAGES, MODEL_NAME
                ): kw for kw in keywords
            }
            
            for future in as_completed(future_to_kw):
                kw = future_to_kw[future]
                try:
                    result = future.result()
                except Exception as e:
                    result = {
                        "keyword": kw,
                        "error": str(e),
                        "serp_df": None,
                        "strategy": None
                    }
                
                all_results[kw] = result
                completed_count += 1
                
                progress_bar.progress(completed_count / len(keywords))
                status_text.text(f"âœ… å®Œæˆï¼š{kw} ({completed_count}/{len(keywords)})")
        
        total_time = time.time() - total_start_time
        
        status_header.success(f"âœ… SERP åˆ†æå®Œæˆï¼ç¸½è€—æ™‚ {total_time:.1f} ç§’")
        status_text.empty()
        
        # åŸ·è¡Œçµ±è¨ˆ
        with st.expander("ğŸ“Š åŸ·è¡Œçµ±è¨ˆ", expanded=False):
            stat_cols = st.columns(4)
            with stat_cols[0]:
                st.metric("SERP å‘¼å«æ¬¡æ•¸", executor.stats["serp_calls"])
            with stat_cols[1]:
                st.metric("Gemini å‘¼å«æ¬¡æ•¸", executor.stats["gemini_calls"])
            with stat_cols[2]:
                st.metric("Gemini é‡è©¦æ¬¡æ•¸", executor.stats["gemini_retries"])
            with stat_cols[3]:
                st.metric("ç¸½è€—æ™‚", f"{total_time:.1f}s")
            
            if executor.stats["errors"]:
                st.warning(f"ç™¼ç”Ÿ {len(executor.stats['errors'])} å€‹éŒ¯èª¤")
                for err in executor.stats["errors"]:
                    st.text(err)
        
        st.divider()
        
        # é¡¯ç¤ºçµæœ
        reports = []
        serp_all_rows = []
        
        for kw in keywords:
            r = all_results.get(kw)
            if not r:
                continue
            
            st.subheader(f"ğŸ” {kw}")
            
            if r.get("timing"):
                timing = r["timing"]
                st.caption(f"â±ï¸ SERP: {timing.get('serp', 0):.1f}s ï½œ Gemini: {timing.get('gemini', 0):.1f}s")
            
            if r.get("error"):
                st.error(f"âŒ è™•ç†å¤±æ•—ï¼š{r['error']}")
                st.divider()
                continue
            
            df = r.get("serp_df")
            strategy = r.get("strategy")
            
            # æ”¶é›† SERP åŸå§‹è³‡æ–™
            if df is not None and not df.empty:
                serp_copy = df.copy()
                serp_copy.insert(0, "Keyword", kw)
                serp_all_rows.append(serp_copy)
            
            # æˆ°å ´åˆ†å¸ƒ
            with st.expander("ğŸ“Š æˆ°å ´åˆ†å¸ƒ", expanded=True):
                col1, col2 = st.columns([2, 1])
                with col1:
                    if df is not None:
                        st.dataframe(
                            df[["Rank", "Type", "Title", "DisplayLink"]], 
                            use_container_width=True, 
                            height=220
                        )
                with col2:
                    if df is not None and not df.empty:
                        type_counts = df["Type"].value_counts().reset_index()
                        type_counts.columns = ["Type", "Count"]
                        chart = alt.Chart(type_counts).mark_arc(innerRadius=50).encode(
                            theta="Count",
                            color="Type",
                            tooltip=["Type", "Count"]
                        )
                        st.altair_chart(chart, use_container_width=True)
            
            # ç­–ç•¥çµè«–
            if strategy and "error" not in strategy:
                st.markdown("### ğŸ§  ç­–ç•¥çµè«–")
                
                col_a, col_b = st.columns(2)
                with col_a:
                    st.info(f"**ä½¿ç”¨è€…æ„åœ–**\n{strategy.get('User_Intent', 'N/A')}")
                    st.success(f"**æ©Ÿæœƒç¼ºå£**\n{strategy.get('Opportunity_Gap', 'N/A')}")
                with col_b:
                    st.warning(f"**æˆ°å ´ç‹€æ…‹**\n{strategy.get('Battlefield_Status', 'N/A')}")
                    st.info(f"**å»ºè­°é å‹**\n{strategy.get('Recommended_Page_Type', 'N/A')}")

                st.markdown("**è‡´å‹åˆ‡è§’**")
                for a in strategy.get("Winning_Angles", []):
                    st.markdown(f"- **{a.get('angle', '')}**ï¼ˆ{a.get('target', '')}ï¼‰")

                st.markdown("**å¿…å‹æ¨™é¡Œ**")
                for t in strategy.get("Killer_Titles", []):
                    st.markdown(f"- {t.get('title', '')}ï½œ{t.get('reason', '')}")

                strategy["Keyword"] = kw
                reports.append(strategy)
            
            elif strategy and "error" in strategy:
                st.error("âŒ ç­–ç•¥è§£æå¤±æ•—")
                with st.expander("æŸ¥çœ‹åŸå§‹å›æ‡‰"):
                    st.code(r.get("raw_response", "N/A"))
            
            st.divider()
        
        # =================================================
        # å…§å®¹å¯«ä½œæ–¹å‘ç¶œåˆæŒ‡å¼•ï¼ˆæ–°åŠŸèƒ½ï¼‰
        # =================================================
        if reports:
            st.header("ğŸ“ å…§å®¹å¯«ä½œæ–¹å‘ç¶œåˆæŒ‡å¼•")
            
            with st.spinner("ğŸ¤– AI æ­£åœ¨ç”¢ç”Ÿå…§å®¹ç­–ç•¥å»ºè­°..."):
                content_direction, error = generate_content_direction(
                    GEMINI_API_KEY, reports, keywords, MODEL_NAME
                )
            
            if error:
                st.error(f"âŒ {error}")
            elif content_direction:
                # æ ¸å¿ƒä¸»é¡Œ
                st.markdown("### ğŸ¯ æ ¸å¿ƒä¸»é¡Œæ–¹å‘")
                st.info(content_direction.get("content_theme", "N/A"))
                
                # ç›®æ¨™å—çœ¾
                st.markdown("### ğŸ‘¥ ç›®æ¨™å—çœ¾")
                st.success(content_direction.get("target_audience", "N/A"))
                
                # å»ºè­°æ–‡ç« æ¶æ§‹
                st.markdown("### ğŸ“ å»ºè­°æ–‡ç« æ¶æ§‹")
                for section in content_direction.get("content_structure", []):
                    with st.container():
                        st.markdown(f"**{section.get('section', '')}**")
                        st.write(section.get('focus', ''))
                        if section.get('keywords_to_use'):
                            st.caption(f"å»ºè­°ä½¿ç”¨é—œéµå­—ï¼š{', '.join(section['keywords_to_use'])}")
                        st.markdown("---")
                
                # å¿…é ˆæ¶µè“‹çš„ä¸»é¡Œ
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("### âœ… å¿…é ˆæ¶µè“‹çš„ä¸»é¡Œ")
                    for topic in content_direction.get("must_cover_topics", []):
                        st.markdown(f"- {topic}")
                
                with col2:
                    st.markdown("### âš ï¸ éœ€é¿å…çš„é™·é˜±")
                    for pitfall in content_direction.get("avoid_pitfalls", []):
                        st.markdown(f"- {pitfall}")
                
                # å·®ç•°åŒ–åˆ‡è§’
                st.markdown("### ğŸ’¡ å·®ç•°åŒ–åˆ‡è§’")
                st.warning(content_direction.get("differentiation_angle", "N/A"))
                
                # å…§å®¹æ ¼å¼å»ºè­°
                st.markdown("### ğŸ“„ å»ºè­°å…§å®¹æ ¼å¼")
                st.info(content_direction.get("content_format_suggestion", "N/A"))
            
            st.divider()
        
        # =================================================
        # Excel è¼¸å‡º
        # =================================================
        if reports:
            st.subheader("ğŸ“¥ ä¸‹è¼‰å ±å‘Š")
            
            # ç­–ç•¥å·¥ä½œè¡¨
            strategy_rows = []
            for r in reports:
                strategy_rows.append({
                    "Keyword": r.get("Keyword", ""),
                    "User_Intent": r.get("User_Intent", ""),
                    "Battlefield_Status": r.get("Battlefield_Status", ""),
                    "Opportunity_Gap": r.get("Opportunity_Gap", ""),
                    "Recommended_Page_Type": r.get("Recommended_Page_Type", ""),
                    "Winning_Angles": "\n".join(
                        [f"- {a.get('angle', '')}ï¼ˆ{a.get('target', '')}ï¼‰"
                         for a in r.get("Winning_Angles", [])]
                    ),
                    "Killer_Titles": "\n".join(
                        [f"- {t.get('title', '')}ï½œ{t.get('reason', '')}"
                         for t in r.get("Killer_Titles", [])]
                    ),
                    "Raw_JSON": json.dumps(r, ensure_ascii=False)
                })

            df_strategy = pd.DataFrame(strategy_rows)
            
            # SERP åŸå§‹è³‡æ–™å·¥ä½œè¡¨
            df_serp_all = pd.concat(serp_all_rows, ignore_index=True) if serp_all_rows else pd.DataFrame()
            
            # å…§å®¹æŒ‡å¼•å·¥ä½œè¡¨
            df_content_direction = pd.DataFrame()
            if content_direction:
                df_content_direction = pd.DataFrame([{
                    "Content_Theme": content_direction.get("content_theme", ""),
                    "Target_Audience": content_direction.get("target_audience", ""),
                    "Differentiation_Angle": content_direction.get("differentiation_angle", ""),
                    "Content_Format": content_direction.get("content_format_suggestion", ""),
                    "Must_Cover_Topics": "\n".join(content_direction.get("must_cover_topics", [])),
                    "Avoid_Pitfalls": "\n".join(content_direction.get("avoid_pitfalls", [])),
                    "Content_Structure": json.dumps(content_direction.get("content_structure", []), ensure_ascii=False)
                }])

            # å¯«å…¥ Excel
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine="xlsxwriter") as writer:
                df_strategy.to_excel(writer, sheet_name="Strategy", index=False)
                
                if not df_serp_all.empty:
                    df_serp_all.to_excel(writer, sheet_name="SERP_Raw", index=False)
                
                if not df_content_direction.empty:
                    df_content_direction.to_excel(writer, sheet_name="Content_Direction", index=False)
                
                # èª¿æ•´æ¬„å¯¬
                workbook = writer.book
                for sheet_name in writer.sheets:
                    worksheet = writer.sheets[sheet_name]
                    worksheet.set_column('A:A', 20)
                    worksheet.set_column('B:H', 40)

            col_dl1, col_dl2 = st.columns(2)
            with col_dl1:
                st.download_button(
                    label="ğŸ“Š ä¸‹è¼‰å®Œæ•´ Excel å ±å‘Š",
                    data=buffer.getvalue(),
                    file_name=f"seo_strategy_{int(time.time())}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
            with col_dl2:
                # JSON å‚™ä»½
                full_json = {
                    "strategies": reports,
                    "content_direction": content_direction
                }
                json_data = json.dumps(full_json, ensure_ascii=False, indent=2)
                st.download_button(
                    label="ğŸ“„ ä¸‹è¼‰ JSON å‚™ä»½",
                    data=json_data,
                    file_name=f"seo_strategy_{int(time.time())}.json",
                    mime="application/json"
                )
