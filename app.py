import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
import google.generativeai as genai
import time
import random
import json

# --- 1. é é¢åŸºç¤è¨­å®š ---
st.set_page_config(
    page_title="Google SERP æˆ°ç•¥é›·é” v2.0 (Pro)",
    page_icon="ğŸŒ",
    layout="wide"
)

st.title("ğŸŒ Google SERP æˆ°ç•¥é›·é” v2.0")
st.markdown("""
### Private SEO Weapon: Localized Intent Analysis
æ­¤å·¥å…·é€é Google Custom Search API æŠ“å–çœŸå¯¦æœå°‹çµæœ (SERP)ï¼Œä¸¦åˆ©ç”¨ Gemini é€²è¡Œæ„åœ–è§£ç¢¼èˆ‡å…§å®¹ç¼ºå£åˆ†æã€‚
""")

# --- 2. å´é‚Šæ¬„ï¼šè¨­å®šèˆ‡é‡‘é‘° ---
with st.sidebar:
    st.header("ğŸ”‘ å•Ÿå‹•é‡‘é‘°")
    st.info("è«‹ç¢ºä¿å·²å•Ÿç”¨ Google Custom Search API")
    GOOGLE_API_KEY = st.text_input("Google API Key", type="password")
    
    # [é˜²å‘†æ©Ÿåˆ¶] è‡ªå‹•ç§»é™¤ä½¿ç”¨è€…å¯èƒ½ä¸å°å¿ƒè²¼ä¸Šçš„ "cx=" å‰ç¶´
    raw_cx = st.text_input("Search Engine ID (CX)", type="password")
    SEARCH_ENGINE_ID = raw_cx.replace("cx=", "").strip() if raw_cx else ""
    
    GEMINI_API_KEY = st.text_input("Gemini API Key", type="password")

    st.divider()
    st.header("ğŸ§  æ¨¡å‹è¨­å®š")
    MODEL_NAME = st.selectbox(
        "é¸æ“‡ AI æ¨¡å‹",
        ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-3-pro-preview"],
        index=0,
        help="Flash é€Ÿåº¦å¿«ä¸”ä¾¿å®œï¼›Pro æ¨ç†èƒ½åŠ›å¼·ï¼›3.0 Preview ç‚ºæœ€æ–°æœ€å¼·å¤§æ¨¡å‹ (éœ€æ³¨æ„ API é…é¡)"
    )

    st.divider()
    st.header("ğŸŒ æˆ°å ´è¨­å®š")
    TARGET_GL = st.text_input("åœ°å€ (gl)", value="tw", help="æœå°‹çµæœçš„åœ°ç†ä½ç½®ï¼Œä¾‹å¦‚: tw, us, jp")
    TARGET_HL = st.text_input("èªè¨€ (hl)", value="zh-TW", help="ä»‹é¢èªè¨€ï¼Œä¾‹å¦‚: zh-TW, en")
    MAX_PAGES = st.slider("æŠ“å–é æ•¸", 1, 3, 2, help="1é =Top10, 2é =Top20 (æ³¨æ„ï¼šæ¯å¤šä¸€é æœƒæ¶ˆè€—ä¸€æ¬¡ API Quota)")

# --- 3. è¼”åŠ©åŠŸèƒ½ï¼šé é¢é¡å‹åµæ¸¬ ---
def detect_page_type(item):
    """æ ¹æ“š URL ç‰¹å¾µèˆ‡ Snippet çµæ§‹ï¼Œç°¡å–®åˆ¤æ–·é é¢å±¬æ€§"""
    link = item.get('link', '').lower()
    
    # ç‰¹å¾µé—œéµå­—åº«
    if any(x in link for x in ['forum', 'ptt.cc', 'dcard.tw', 'mobile01', 'reddit']):
        return "ğŸ—£ï¸ Forum (è«–å£‡/UGC)"
    if any(x in link for x in ['/product/', 'shopee', 'momo', 'pchome', 'amazon', 'rakuten']):
        return "ğŸ›’ E-commerce (é›»å•†)"
    if any(x in link for x in ['/news/', 'news.', 'udn.com', 'ltn.com']):
        return "ğŸ“° News (æ–°è)"
    if '.gov' in link:
        return "ğŸ›ï¸ Government (æ”¿åºœ)"
    if 'wiki' in link or 'wikipedia' in link:
        return "ğŸ“– Wiki (ç¶­åŸº)"
    if 'blog' in link or 'article' in link:
        return "ğŸ“ Blog (éƒ¨è½æ ¼)"
        
    return "ğŸ“„ General (ä¸€èˆ¬é é¢)"

# --- 4. æ ¸å¿ƒåŠŸèƒ½ï¼šGoogle SERP çˆ¬èŸ² (å« Retry æ©Ÿåˆ¶) ---
def get_google_serp_data(api_key, cx, keyword, gl='tw', hl='zh-TW', pages=1):
    service = build("customsearch", "v1", developerKey=api_key)
    all_results = []
    
    # é€²åº¦æ¢ (é¡¯ç¤ºåœ¨ä¸»ç•«é¢)
    status_text = st.empty()
    
    for page in range(pages):
        start_index = (page * 10) + 1  # Google API åˆ†é é‚è¼¯: 1, 11, 21...
        retries = 3
        
        status_text.text(f"æ­£åœ¨æŠ“å–ç¬¬ {page + 1} é  (Rank {start_index}-{start_index+9})...")
        
        while retries > 0:
            try:
                res = service.cse().list(
                    q=keyword,
                    cx=cx,
                    num=10,
                    start=start_index,
                    gl=gl,
                    hl=hl
                ).execute()
                
                items = res.get('items', [])
                if not items:
                    break 
                
                for i, item in enumerate(items):
                    # å˜—è©¦æå–æ›´è±å¯Œçš„æè¿° (og:description å„ªå…ˆ)
                    pagemap = item.get('pagemap', {})
                    metatags = pagemap.get('metatags', [{}])[0]
                    description = metatags.get('og:description', item.get('snippet'))
                    
                    all_results.append({
                        "Rank": start_index + i,
                        "Type": detect_page_type(item),
                        "Title": item.get('title'),
                        "Description": description,
                        "DisplayLink": item.get('displayLink'),
                        "Link": item.get('link')
                    })
                break # æˆåŠŸå‰‡è·³å‡º retry
                
            except Exception as e:
                retries -= 1
                wait_time = (3 - retries) * 2 + random.uniform(0, 1) # Exponential Backoff
                st.warning(f"é€£ç·šä¸ç©©ï¼Œç¬¬ {3-retries} æ¬¡é‡è©¦ä¸­... ({e})")
                time.sleep(wait_time)
                if retries == 0:
                    st.error(f"âŒ ç„¡æ³•æŠ“å–ç¬¬ {page+1} é : {e}")
        
        time.sleep(1) # é¿å…è§¸ç™¼ Rate Limit
        
    status_text.empty() # æ¸…é™¤ç‹€æ…‹æ–‡å­—
    return all_results

# --- 5. æ ¸å¿ƒåŠŸèƒ½ï¼šGemini æ„åœ–åˆ†æ (JSON è¼¸å‡º) ---
def analyze_intent_with_gemini(api_key, keyword, df, gl, model_name):
    genai.configure(api_key=api_key)
    # [æ›´æ–°] ä½¿ç”¨ä½¿ç”¨è€…é¸æ“‡çš„æ¨¡å‹
    model = genai.GenerativeModel(model_name)
    
    # ç²¾ç°¡è³‡æ–™ä»¥ç¯€çœ Token
    data_str = df[['Rank', 'Type', 'Title', 'Description', 'DisplayLink']].to_string(index=False)
    
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ˆç²¾æ–¼ SEO çš„æˆ°ç•¥é¡§å•ã€‚æˆ‘å€‘æ­£åœ¨åˆ†æé—œéµå­—ã€Œ{keyword}ã€åœ¨ Google æœå°‹çµæœ ({gl} åœ°å€) çš„å‰ {len(df)} ååˆ†ä½ˆã€‚
    
    ä»¥ä¸‹æ˜¯ç«¶çˆ­å°æ‰‹æ•¸æ“š (Type ç‚ºåˆæ­¥åˆ¤æ–·çš„é é¢é¡å‹)ï¼š
    {data_str}
    
    è«‹é€²è¡Œæ·±åº¦çš„æœå°‹æ„åœ– (Search Intent) è§£ç¢¼ï¼Œä¸¦åš´æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡º (ä¸è¦åŒ…å« Markdown ```json æ¨™è¨˜ï¼Œç›´æ¥è¼¸å‡º JSON å­—ä¸²)ï¼š
    
    {{
        "User_Intent_Analysis": "ä½¿ç”¨è€…æ„åœ–åˆ†æ (ä¾‹å¦‚ï¼šä»–å€‘ä¸»è¦æ˜¯æƒ³æ¯”åƒ¹ã€æ‰¾æ•™å­¸ã€é‚„æ˜¯æ‰¾è©•åƒ¹ï¼Ÿ)",
        "Market_Landscape": "ç›®å‰æˆ°å ´æ¦‚æ³ (ä¾‹å¦‚ï¼šé›»å•†éœ¸æ¦œã€è«–å£‡è¨è«–åº¦é«˜ã€é‚„æ˜¯è¢«å¤§åª’é«”å£Ÿæ–·ï¼Ÿ)",
        "Content_Gap": "å…§å®¹ç¼ºå£ç™¼ç¾ (å‰å¹¾åæœ‰ä»€éº¼ç—›é»æ²’è¬›æ¸…æ¥šï¼Ÿæˆ–æ˜¯ Rank é å¾Œä½†å…§å®¹å¾ˆå¥½çš„éºç ï¼Ÿ)",
        "Winning_Strategy": "é™ç¶­æ‰“æ“Šç­–ç•¥ (å¦‚æœæˆ‘å€‘è¦è´ï¼Œè©²æ¡å–ä»€éº¼ç¨ç‰¹åˆ‡è§’ï¼Ÿ)",
        "Killer_Titles": [
            {{ "title": "å¿…å‹æ¨™é¡Œ1", "reason": "ç‚ºä»€éº¼é€™å€‹æ¨™é¡Œèƒ½è´ï¼Ÿ" }},
            {{ "title": "å¿…å‹æ¨™é¡Œ2", "reason": "ç‚ºä»€éº¼é€™å€‹æ¨™é¡Œèƒ½è´ï¼Ÿ" }},
            {{ "title": "å¿…å‹æ¨™é¡Œ3", "reason": "ç‚ºä»€éº¼é€™å€‹æ¨™é¡Œèƒ½è´ï¼Ÿ" }}
        ]
    }}
    """
    
    # [ä¿®æ­£] åˆå§‹åŒ– response è®Šæ•¸ï¼Œé¿å… UnboundLocalError
    response = None
    
    try:
        response = model.generate_content(prompt)
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ markdown æ¨™è¨˜
        clean_text = response.text.strip()
        if clean_text.startswith("```json"):
            clean_text = clean_text[7:]
        if clean_text.endswith("```"):
            clean_text = clean_text[:-3]
        return json.loads(clean_text)
    except Exception as e:
        # [ä¿®æ­£] æ›´å®‰å…¨çš„éŒ¯èª¤è™•ç†é‚è¼¯
        raw_text_content = "ç„¡å›æ‡‰å…§å®¹"
        if response:
            try:
                raw_text_content = response.text
            except:
                raw_text_content = "ç„¡æ³•è®€å–å›æ‡‰æ–‡å­—"
                
        return {"error": f"AI è§£æå¤±æ•—: {str(e)}", "raw_text": raw_text_content}

# --- 6. ä¸»ç¨‹å¼åŸ·è¡Œé‚è¼¯ ---
keywords_input = st.text_area("è¼¸å…¥é—œéµå­— (ä¸€è¡Œä¸€å€‹)", height=100, placeholder="ç©ºæ°£æ¸…æ·¨æ©Ÿ æ¨è–¦\nCRM ç³»çµ±æ¯”è¼ƒ\nå°åŒ— ç‡’è‚‰ 2025")

if st.button("ğŸš€ å•Ÿå‹•æˆ°ç•¥é›·é”", type="primary"):
    # æª¢æŸ¥ Key
    if not (GOOGLE_API_KEY and SEARCH_ENGINE_ID and GEMINI_API_KEY):
        st.warning("âš ï¸ è«‹å…ˆåœ¨å·¦å´æ¬„ä½è¼¸å…¥æ‰€æœ‰ API Key")
        st.stop()
        
    if not keywords_input.strip():
        st.warning("âš ï¸ è«‹è¼¸å…¥è‡³å°‘ä¸€å€‹é—œéµå­—")
        st.stop()

    keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]
    
    # ç¸½é«”é€²åº¦æ¢
    main_progress = st.progress(0)
    
    # å»ºç«‹ä¸€å€‹åˆ—è¡¨ä¾†å„²å­˜æ‰€æœ‰å ±å‘Šæ•¸æ“š
    report_data = []
    
    for idx, kw in enumerate(keywords):
        st.subheader(f"ğŸ” åˆ†æç›®æ¨™ï¼š{kw}")
        
        # 1. æŠ“å–è³‡æ–™
        with st.spinner(f"æ­£åœ¨æƒæ Google SERP (Top {MAX_PAGES*10})..."):
            raw_data = get_google_serp_data(GOOGLE_API_KEY, SEARCH_ENGINE_ID, kw, TARGET_GL, TARGET_HL, MAX_PAGES)
            
        if raw_data:
            df = pd.DataFrame(raw_data)
            
            # 2. é¡¯ç¤ºæ•¸æ“š
            with st.expander(f"ğŸ“Š {kw} - SERP æˆ°å ´æ•¸æ“š (é»æ“Šå±•é–‹)", expanded=False):
                st.dataframe(df, use_container_width=True)
            
            # 3. AI åˆ†æ
            with st.spinner(f"ğŸ§  {MODEL_NAME} æ­£åœ¨è¨ˆç®—æˆ°ç•¥ ({kw})..."):
                # [æ›´æ–°] å‚³å…¥ MODEL_NAME
                analysis_result = analyze_intent_with_gemini(GEMINI_API_KEY, kw, df, TARGET_GL, MODEL_NAME)
                
                if "error" in analysis_result:
                    st.error(f"âŒ {analysis_result['error']}")
                    # åªæœ‰ç•¶æœ‰åŸå§‹æ–‡å­—æ™‚æ‰é¡¯ç¤ºï¼Œé¿å…ç•«é¢æ··äº‚
                    if analysis_result["raw_text"] != "ç„¡å›æ‡‰å…§å®¹":
                        st.text(f"Raw Output: {analysis_result['raw_text']}")
                else:
                    # ç¾åŒ–è¼¸å‡º
                    st.markdown("#### ğŸ“ æˆ°ç•¥åˆ†æå ±å‘Š")
                    
                    c1, c2 = st.columns(2)
                    with c1:
                        st.info(f"**ä½¿ç”¨è€…æ„åœ–ï¼š**\n{analysis_result.get('User_Intent_Analysis', '')}")
                    with c2:
                        st.warning(f"**æˆ°å ´æ¦‚æ³ï¼š**\n{analysis_result.get('Market_Landscape', '')}")
                        
                    st.success(f"**ğŸ’¡ å…§å®¹ç¼ºå£èˆ‡æ©Ÿæœƒï¼š**\n{analysis_result.get('Content_Gap', '')}")
                    
                    st.markdown("##### ğŸ¯ é™ç¶­æ‰“æ“Šç­–ç•¥")
                    st.write(analysis_result.get('Winning_Strategy', ''))
                    
                    st.markdown("##### ğŸ† å»ºè­°å¿…å‹æ¨™é¡Œ")
                    for t in analysis_result.get('Killer_Titles', []):
                        st.markdown(f"- **{t['title']}**\n  - *{t['reason']}*")

                    # [æ–°å¢] æ”¶é›†æ•¸æ“šä¾›ä¸‹è¼‰
                    titles_formatted = "\n".join([f"- {t['title']} ({t['reason']})" for t in analysis_result.get('Killer_Titles', [])])
                    report_data.append({
                        "Keyword": kw,
                        "User_Intent_Analysis": analysis_result.get('User_Intent_Analysis', ''),
                        "Market_Landscape": analysis_result.get('Market_Landscape', ''),
                        "Content_Gap": analysis_result.get('Content_Gap', ''),
                        "Winning_Strategy": analysis_result.get('Winning_Strategy', ''),
                        "Killer_Titles": titles_formatted
                    })
        else:
            st.error(f"âŒ ç„¡æ³•æŠ“å– {kw} çš„è³‡æ–™ï¼Œè«‹æª¢æŸ¥ API é…é¡ã€‚")
            
        st.divider()
        main_progress.progress((idx + 1) / len(keywords))
        
    st.success("âœ… æ‰€æœ‰é—œéµå­—åˆ†æå®Œæˆï¼")

    # [æ–°å¢] ä¸‹è¼‰å€å¡Š
    if report_data:
        st.header("ğŸ“¥ ä¸‹è¼‰æˆ°ç•¥å ±å‘Š")
        st.caption("å°‡æ‰€æœ‰åˆ†æçµæœåŒ¯å‡ºä¿å­˜")
        
        # æº–å‚™ DataFrame
        df_report = pd.DataFrame(report_data)
        
        # ç”¢ç”Ÿ CSV (ä½¿ç”¨ utf-8-sig ä»¥ç¢ºä¿ Excel é–‹å•Ÿä¸­æ–‡ä¸äº‚ç¢¼)
        csv_data = df_report.to_csv(index=False).encode('utf-8-sig')
        
        # ç”¢ç”Ÿ JSON
        json_data = json.dumps(report_data, ensure_ascii=False, indent=2)
        
        col_d1, col_d2 = st.columns(2)
        
        with col_d1:
            st.download_button(
                label="ğŸ“„ ä¸‹è¼‰ Excel å‹å–„ CSV",
                data=csv_data,
                file_name=f"seo_strategy_report_{int(time.time())}.csv",
                mime="text/csv"
            )
            
        with col_d2:
            st.download_button(
                label="ğŸ“‹ ä¸‹è¼‰ JSON (å®Œæ•´çµæ§‹)",
                data=json_data,
                file_name=f"seo_strategy_report_{int(time.time())}.json",
                mime="application/json"
            )
