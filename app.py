import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
import google.generativeai as genai
import time
import random
import json
import altair as alt

# --- 1. é é¢åŸºç¤è¨­å®š ---
st.set_page_config(
    page_title="Google SERP æˆ°ç•¥é›·é” v3.0 (Enterprise)",
    page_icon="ğŸ¯",
    layout="wide"
)

st.title("ğŸ¯ Google SERP æˆ°ç•¥é›·é” v3.0")
st.markdown("""
### Private SEO Weapon: Battlefield Reader & Content Architect
ä¸åƒ…æ˜¯åˆ†ææ„åœ–ï¼Œæ›´ç›´æ¥ç”Ÿæˆã€Œå¯è½åœ°çš„å…§å®¹ç­–ç•¥ã€èˆ‡ã€Œå¯«ä½œå¤§ç¶±ã€ã€‚å…·å‚™è‡ªå‹•ä¿®å¾© JSON èˆ‡æˆæœ¬ç›£æ§åŠŸèƒ½ã€‚
""")

# --- 2. å´é‚Šæ¬„ï¼šè¨­å®šèˆ‡é‡‘é‘° ---
with st.sidebar:
    st.header("ğŸ”‘ å•Ÿå‹•é‡‘é‘°")
    st.info("è«‹ç¢ºä¿å·²å•Ÿç”¨ Google Custom Search API")
    GOOGLE_API_KEY = st.text_input("Google API Key", type="password")
    
    # [é˜²å‘†] è‡ªå‹•ç§»é™¤ cx=
    raw_cx = st.text_input("Search Engine ID (CX)", type="password")
    SEARCH_ENGINE_ID = raw_cx.replace("cx=", "").strip() if raw_cx else ""
    
    GEMINI_API_KEY = st.text_input("Gemini API Key", type="password")

    st.divider()
    st.header("ğŸ§  æ¨¡å‹è¨­å®š")
    MODEL_NAME = st.selectbox(
        "é¸æ“‡ä¸»è¦åˆ†ææ¨¡å‹",
        ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-3-pro-preview"],
        index=0,
        help="å»ºè­°ï¼šç”¨ Flash è·‘å¤§é‡æ¸¬è©¦ï¼Œç”¨ 3.0 Pro ç”¢å‡ºæœ€çµ‚é«˜æ™ºå•†ç­–ç•¥ã€‚"
    )

    st.divider()
    st.header("ğŸŒ æˆ°å ´è¨­å®š")
    TARGET_GL = st.text_input("åœ°å€ (gl)", value="tw", help="ä¾‹å¦‚: tw, us, jp")
    TARGET_HL = st.text_input("èªè¨€ (hl)", value="zh-TW", help="ä¾‹å¦‚: zh-TW, en")
    MAX_PAGES = st.slider("æŠ“å–é æ•¸", 1, 3, 2, help="1é =Top10, 2é =Top20 (æ³¨æ„é…é¡æ¶ˆè€—)")

# --- 3. æ ¸å¿ƒå·¥å…·å‡½å¼åº« ---

def detect_page_type(item):
    """
    [å‡ç´š] æ›´ç´°ç·»çš„é é¢é¡å‹åˆ¤æ–·é‚è¼¯
    å€åˆ†ï¼šé›»å•†ã€åª’é«”ã€è«–å£‡ã€å®˜ç¶²ã€æ”¿åºœ/ç¶­åŸºã€éƒ¨è½æ ¼
    """
    link = item.get('link', '').lower()
    snippet = item.get('snippet', '').lower()
    title = item.get('title', '').lower()
    
    # å¼·ç‰¹å¾µåˆ¤æ–·
    if any(x in link for x in ['forum', 'ptt.cc', 'dcard.tw', 'mobile01', 'reddit', 'baha']):
        return "ğŸ—£ï¸ UGC/Forum (è«–å£‡)"
    if any(x in link for x in ['youtube.com', 'instagram.com', 'facebook.com', 'tiktok.com']):
        return "ğŸ¥ Social/Video (ç¤¾ç¾¤å½±éŸ³)"
    if any(x in link for x in ['/product/', 'shopee', 'momo', 'pchome', 'amazon', 'rakuten', 'buy123']):
        return "ğŸ›’ E-commerce (é›»å•†)"
    if any(x in link for x in ['/news/', 'news.', 'udn.com', 'ltn.com', 'chinatimes', 'ettoday']):
        return "ğŸ“° Media/News (æ–°èåª’é«”)"
    if '.gov' in link:
        return "ğŸ›ï¸ Government (æ”¿åºœ)"
    if 'wiki' in link or 'wikipedia' in link:
        return "ğŸ“– Wiki (ç™¾ç§‘)"
    
    # å¼±ç‰¹å¾µåˆ¤æ–· (ä¾æ“šæ¨™é¡Œæˆ– snippet)
    if any(x in title for x in ['åƒ¹æ ¼', 'å„ªæƒ ', 'è²·', 'æŠ˜æ‰£', 'price', 'shop']):
        return "ğŸ›’ E-commerce (ç–‘ä¼¼é›»å•†)"
    if any(x in link for x in ['blog', 'article', 'post', 'topic']):
        return "ğŸ“ Blog/Article (å…§å®¹é )"
        
    return "ğŸ“„ General (ä¸€èˆ¬é é¢)"

# [å‡ç´š] åŠ ä¸Šå¿«å–æ©Ÿåˆ¶ï¼Œé¿å…é‡è¤‡æ‰£ Quota
@st.cache_data(ttl=3600, show_spinner=False)
def get_google_serp_data_cached(api_key, cx, keyword, gl, hl, pages):
    """
    å¿«å–ç‰ˆçš„ SERP æŠ“å–å™¨ã€‚
    åªè¦åƒæ•¸ (keyword, gl, hl, pages) ç›¸åŒï¼Œä¸€å°æ™‚å…§ä¸æœƒé‡è¤‡ call Google APIã€‚
    """
    # å»ºç«‹ service ç‰©ä»¶ (ç„¡æ³• pickleï¼Œæ‰€ä»¥ä¸å¿«å– service æœ¬èº«ï¼Œåªå¿«å–çµæœ)
    try:
        service = build("customsearch", "v1", developerKey=api_key)
    except Exception as e:
        return {"error": f"Service Build Error: {e}"}

    all_results = []
    
    for page in range(pages):
        start_index = (page * 10) + 1
        retries = 3
        
        while retries > 0:
            try:
                res = service.cse().list(
                    q=keyword,
                    cx=cx,
                    num=10,
                    start=start_index,
                    gl=gl,
                    hl=hl
                ).execute()
                
                items = res.get('items', [])
                if not items:
                    break 
                
                for i, item in enumerate(items):
                    # å˜—è©¦æŠ“å– og:description
                    pagemap = item.get('pagemap', {})
                    metatags = pagemap.get('metatags', [{}])[0]
                    description = metatags.get('og:description', item.get('snippet'))
                    
                    # [å„ªåŒ–] æˆªæ–·éé•·çš„æè¿°ä»¥ç¯€çœ Token
                    if description and len(description) > 200:
                        description = description[:200] + "..."

                    all_results.append({
                        "Rank": start_index + i,
                        "Type": detect_page_type(item),
                        "Title": item.get('title'),
                        "Description": description,
                        "DisplayLink": item.get('displayLink'),
                        "Link": item.get('link')
                    })
                break
            except Exception as e:
                retries -= 1
                wait_time = (3 - retries) * 2 + random.uniform(0, 1)
                time.sleep(wait_time)
                if retries == 0:
                    return {"error": f"API Fetch Error (Page {page+1}): {str(e)}"}
        
        time.sleep(1.5) # ç¨å¾®ä¼‘æ¯
        
    return all_results

def repair_json_with_gemini(api_key, broken_text, error_msg):
    """
    [æ–°å¢] JSON å¤–ç§‘æ‰‹è¡“ä¿®å¾©å¸«
    ç•¶ä¸»è¦æ¨¡å‹åå‡ºçˆ›æ‰çš„ JSON æ™‚ï¼Œå‘¼å«ä¾¿å®œçš„ Flash æ¨¡å‹ä¾†ä¿®å¾©å®ƒã€‚
    """
    genai.configure(api_key=api_key)
    # ä½¿ç”¨ Flash ä¿®å¾©ï¼Œé€Ÿåº¦å¿«ä¸”ä¾¿å®œ
    repair_model = genai.GenerativeModel('gemini-2.5-flash')
    
    prompt = f"""
    You are a JSON repair expert. The following text was intended to be a valid JSON but failed to parse.
    Error: {error_msg}
    
    Broken Text:
    {broken_text}
    
    Please fix the JSON structure, remove any markdown formatting (like ```json), and return ONLY the valid JSON string.
    Do not add any explanations.
    """
    try:
        response = repair_model.generate_content(prompt)
        cleaned = response.text.strip()
        if cleaned.startswith("
